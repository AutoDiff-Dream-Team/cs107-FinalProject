{"cells":[{"cell_type":"markdown","source":"## Automatic Differentiation Documentation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00000-e180ff43-6804-45d7-9bf0-c9755cd2b7e1"}},{"cell_type":"markdown","source":"### Introduction\n\n---\n\nAutomatic differentiation (AD) encompasses a suite of tools used to compute the derivatives of functions, evaluate the functions at specified values, and evaluate the functions' derivates at specified values. In a situation where analytically deriving the derivative of compicated functions is not feasible (within the user's limitations), AD guarantees to return the exact solution in theory. In practice, rounding errors may compound given that AD performs a series of elementary operations.  \n\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00001-c7f7893c-d0be-4d7e-b0d4-3d7c3059a64f"}},{"cell_type":"markdown","source":"### Background\n\n---\n\n\nThe way automatic differentiation works is by taking a possibly complex function and breaking it down into a sequence of elementary functions \n(i.e. summation, multiplication, cosine, etc.), where the output or outputs of past elementary functions are fed into the input of the next elementary \nfunction. The sequence of the elementary functions starts by first assigning a value or values to the variables in the function, \nthen working its way from the inside out of the function by sequentially performing the elementary functions until you build out the whole function. \nThis sequence can be expressed in a graph structure. ![Graph Image](https://blog.paperspace.com/content/images/2019/03/computation_graph_forward.png) \nOnce you have the sequence of elementary functions, what automatic differentiation does is \"passing down\" the evaluation of the elementary function and the evaluation \nof the derivate through the sequence to get the whole function and its derivative evaluated at certain values. Mathematically, to evaluate an elementary function node, \nyou need to take the function evaluation outputs of the nodes that feed into it and use that to evaluate that node. There is more subtlety for passing along the derivative evaluation. \nIn a way, each elementary function has some variables that it depends on, but those variables from the previous nodes depend on other variables and thus, to pass the derivative along \nthe sequence we need to use the chain rule $$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial x}$$. Now we can see that we need \nto take the derivate of the node we are in and multiply it by the evaluated derivative(s) of the previous node(s)! But we still need a derivate evaluated for the initial mode, \nand this will be assigned with a seed vector of the choosing. Intuivitely, what this seed vector is doing is making the derivative into a directional derivative,\nthe seed vector being the direction where the derivative (or the Jacobian in the case of multiple functions) is being projected in.\n\nFor example, a user may want to evaluate the derivative of a complicated function $f'$ at a given point. Let us define a function $f$:\n$$\nf(x) = sin^3(x^2 + cos(\\sqrt{x}))\n$$\nThe derivative of the function $f'$, given below, is messy and tedious to derive.\n$$\nf'(x) = 3 \\left(2x - \\frac{sin(\\sqrt{x})}{2\\sqrt{x}}\\right) cos(x^2 + cos(\\sqrt{x})) sin^2(x^2 + cos(\\sqrt{x})) \n$$\nHowever, the user does not have to analytically derive the derivative of the given function when using automatic differentiation. \nProvided the user supplies a function of interest and point(s) of interest, the derivative of the given function will be evaluated\nat the given point(s) of interest. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-975f81a1-87cd-40b0-a1ef-534344db58d4"}},{"cell_type":"markdown","source":"### Software Organization\n\n---\n\n#### Directory Structure\n\n\n    Auto_diff/\n\n        __init__.py  \n        ad/\n            __init__.py\n            AD.py  # create AD objects\n                     ...\n        utils/ \n            __init__.py \n            jacobian.py # helps create jacobian matrix\n                           ...\n        tests/\n            __init__.py \n            test_basic.py  # test basic operations\n            test_jacobian.py  # test jacobian helper\n                   ...\n                   \n#### Modules\n\nWe have four modules within our package `Auto_diff`.\n*  `AD`: a module that contains the following class:\n    * The `AD` class used for instatiating an AD object, which is able to perform the forward mode of automatic differentiation and produces the numerical output.\n*   `jacobian`: a module that contains the following functions:\n    * The function`Jacobian` used for handling functions of multiple inputs. This function takes as an argument an integer defining the number of inputs for the given function and returns a list of AD objects.\n*  `test_basic`: a module that tests all the elementary functions (addition, multiplication, power, etc) and functions like `get_value` and `get_derivative`. \n*  `test_jacobian`: a module that tests function `Jacobian` on a single function and multiple functions.\n\n\n#### Testing\n\nModule testing can be found in the files `test_basic.py` and `test_jacobian.py`. \n* ** test_basic: ** Each elementary function is tested individually inside `test_basic.py`.\n* ** test_jacobian: ** Function `Jacobian` is tested on a single function and multiple functions respectively to see if the function works in both scenarios.\n\nOur test suite is included in the subdirectory `tests` that runs with pytest automatically on TravisCI and CodeCov. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-856770e8-db83-4028-aae0-bc86db87a814"}},{"cell_type":"markdown","source":"### Installation\n\n##### Installing Python\nYou will need an updated version of python that is compatible with your system. These downloads can be found [here](https://www.python.org/downloads/).\nDownloading a python version $\\geq$ 3.4 will also install pip, the package manager for python.\n\n##### Installing Git\nGit is a version control software that will be used in order to pull all relevant package data from the Github repository. This step is not\nnecessary, but it greatly simplifies the process of downloading all relevant data. The steps used to install git for your given machine\ncan be found [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\nThe automatic differentiation package can be installed by cloning the necessary github repository using the following command.\n\n    git clone https://github.com/AutoDiff-Dream-Team/cs107-FinalProject.git\n\n\nThe dependencies needed to properly use this pacakge can be installed by running the following command. First, you must be located in the directory that contains the `requirements.txt` file.\n\n    pip install -r requirements.txt\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-c38ae4fa-cf96-4380-b604-5a54491958da"}},{"cell_type":"markdown","source":"### Implementation\n---\nClasses: \nThere will be one main class that users will interact with in order to perform AD. The `AD` class is capable of performing the forward mode of automatic differentiation. The basic workflow is as follows: The user instantiates an instance of the `AD` class and uses\nthe newly created `AD` object as the input to a user-defined function. The `AD` object stores the value of the function and the value of the derivative\nin the attributes `val` and `der`. \n\nInputs:\n* **val**\n    * Type: Numeric (default is 1)\n    * The value of the function evaluated at the specified user input\n* **der** \n    * Type: Numeric (default is 1)\n    * The value of the derivative of the function evaluated at the specified user input\n    * This input is likely to be changed from 1 when computing partial derivatives for functions of multiple variables\n\nAttributes:\n* **val**\n    * Type: Numeric (default is initially 1)\n    * The value of the function evaluated at the specified user input\n* **der** \n    * Type: Numeric (default is initially 1)\n    * The value of the derivative of the function evaluated at the specified user input\n\nMethods:\n* The `AD` class does not contain methods that will be commonly accessed by the user.\n* Basic operations including addition, subtraction, multiplication, division, power, exponential, negation, and the trigonometric functions sine, cosine, and tangent\n  are overloaded in the definition of the class.\n\nNecessary Dependencies:\n\n`Numpy` is the only dependency that will be required in order to properly use our package. \n`Numpy` will be used to handle the elementary functions such as $sin(x)$ and $exp(x)$.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-3bd5f617-f224-4423-8452-dd6fa3c36b71"}},{"cell_type":"markdown","source":"### How to Use\n---","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-83187a7e-3aa8-47c7-beb9-b9a29c9e6c78"}},{"cell_type":"markdown","source":"### Instantiating and using an AD object","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-08f4e063-b84f-465f-9725-6f42347030fc"}},{"cell_type":"markdown","source":"Import the AD module and numpy using the following commands. \n\n    \n    from Auto_diff import AD\n    import numpy as np","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-c3dae577-c5c0-4492-8346-c09982ea07d4"}},{"cell_type":"markdown","source":"Instantiate an AD object. You can change the first argument, but leave the second argument as 1 for this example.\n    \n    # Instantiates the AD object\n    val = 3\n    x = AD(val,1)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-95f22758-4f2f-41cd-a90e-a1e7d18066e3"}},{"cell_type":"markdown","source":"Define a function that takes one argument as input. This will be your AD object.\n\n    \n    # User defined function (you can change the body of this function)\n    def f(x):\n        output = np.sin(x**2 + 3)\n        return output","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-9318cef3-44a3-4ebf-af09-9f9de61b19c6"}},{"cell_type":"markdown","source":"Once the function is defined, you can call the function by passing the AD object as the argument.\n\n    \n    # Running this will change the values of the val and der attributes (these are shown in output which is an AD object)\n    output = f(x)\n\n    # If you dont wan't to define a function you can compute them on the fly\n    output = np.sin(x**2 + 3)\n\n    print(f\"The value of the function evaluated at {val} is {output.val}.\")\n    print(f\"The value of the derivative of the function evaluated at {val} is {output.der}.\")","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-2f3d36c8-6b6f-412d-8ffb-a2be12a78b30"}},{"cell_type":"markdown","source":"### Using multiple AD objects (functions of multiple inputs)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-f8ac556d-1538-46d4-898b-31b2accfda1e"}},{"cell_type":"markdown","source":"You can define functions of multiple variables where each variable is an AD object. An example is given below. When using multiple variables\nthe second arguments in the instantiations of the AD objects will define the seed vector and therefore will dictate the value of the computed \nderivative. For example, let us define a function $f=x^2+y$. Let us set the values for $x$ and $y$ as 2 and 3 respectively.\nWe can instantiate the AD objects as follows and define our function as follows.\n\n    # Instantiates the AD objects\n    x = AD(2,1) \n    y = AD(3,1) \n\n    # Define the function\n    def f(x,y):\n        return x**2 + y\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-38a910c5-b89c-4bdb-b47b-7d18da97a066"}},{"cell_type":"markdown","source":"We will first manually derive the value of the function and the values of the partial derivatives of the function.\n\nWe can evaluate the function at $x=2$ and $y=3$. \n$$\nf(2,3) = 2^2 + 3 = 7\n$$\nWe can also determine $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$.\n$$ \n\\frac{\\delta f}{\\delta x} = 2x\n$$\n$$ \n\\frac{\\delta f}{\\delta y} = 1  \n$$\nUsing our values for $x$ and $y$, $\\frac{\\delta f}{\\delta x}=4$ and $\\frac{\\delta f}{\\delta y}=1$.\nWe can compute $\\frac{\\delta f}{\\delta x}$ by setting the second argument for `x` to 1 and the second\nargument for `y` to 0. Similarly, we can compute $\\frac{\\delta f}{\\delta y}$ by setting the second argument \nfor `x` to 0 and the second argument for `y` to 1. This is done as follows.\n\n    # Evaluate the function and the partial derivative with respect to x for x=2 and y=3\n    x = AD(2,1) \n    y = AD(3,0) \n    output1 = f(x,y)\n\n    # Evaluate the function and the partial derivative with respect to y for x=2 and y=3\n    x = AD(2,0) \n    y = AD(3,1) \n    output2 = f(x,y)\n\n    # The value of the function should be the same for both evaluations\n    function_value1 = output1.val\n    function_value2 = output2.val\n    assert function_value1 == function_value2\n\n    der_x = output1.der\n    der_y = output2.der\n\n    print(f\"The value of the function f at x=2 and y=3 is {function_value1}\".)\n    print(f\"The partial derivative with respect to x of f at x=2 and y=3 is {der_x}\".)\n    print(f\"The partial derivative with respect to y of f at x=2 and y=3 is {der_y}\".)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00018-ffab7377-600b-4112-aeff-3155f1ee583e"}},{"cell_type":"markdown","source":"### Computing the Jacobian","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00014-1c5c6856-8007-4432-8012-66db6085c603"}},{"cell_type":"markdown","source":"We also give the user the option of directly taking the Jacobian Matrix of a function or set of functions with one or multiple variables. The user will\nimport the Jacobian module and call the Jacobian method by passing just the values of the variables they want and the method will return a numpy ndarray \nof AD objects ready to be used in a single function or in a list of functions. This will return a matrix of n x n of AD objects evaluated at the \ncorrect seed vectors for the Jacobian. You can get only the derivatives by using the get_derivative method.\n    \n    from Auto_diff import Jacobian, AD\n\n    # Passing in list of values for x amount of variables\n    x = Jacobian([2, 7, 10])\n\n    # This assignment will return a 3x3 matrix of AD objects\n    jacobian_results = [np.exp(x[0]/x[1] + x[2]),tan(sin(3^x[0])*x[2]), x[0]+x[1]+x[2]]\n\n    # Getting only the derivatives of the 3x3 matrix, i.e., the Jacobian matrix\n    jacobian_matrix = AD.get_derivatives(jacobian_results) ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00014-a2599165-4a59-4de2-b0a3-203a091ed219"}},{"cell_type":"markdown","source":"### Future Features\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00020-28df31e3-80fa-4f3b-a20e-41ae22bc75d7"}},{"cell_type":"markdown","source":"We plan to implement the reverse mode of automatic differentiation. This implementation is meant to overcome one of the main pitfalls of \nthe forward mode, namely computing multiple partial derivatives such as has to be done when computing the Jacobian. For example, given \nwe want to compute $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ for some function $f$, we would need to perform the forward\nmode twice. Once when setting the seed for the `AD` object associated with $x$ to 1 and the `AD` object associated with $y$ to 0 and another\ntime setting the seed for the `AD` object associated with $x$ to 0 and the `AD` object associated with $y$ to 1. \n\nWe can think of performing the reverse mode as inverting the expressions for the derivatives when peforming the chain rule. However, this\nwill not necessarily be straighforward to implement. We envision recursively propagating the gradient of the given function down the nodes\nof a tree where the nodes represent each step in the evaluation trace. \n\nImplementing the reverse mode may change how we structure our module. As of now, the `AD` class is strictly used for implementing the forward\nmode. We may define an argument `mode` upon instantiation of an `AD` object that can either be set to \"forward\" or \"reverse\". Setting this\nargument to reverse will instantiate an object that can be used to perform the reverse mode. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00018-92f7a0ec-df71-4e6c-8b20-3306c8c9284e"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00021-93d9e160-52c8-4f0c-9f6e-7754ff32dea8"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-3e5d9a16-3d22-44b8-8777-57237699e9fb"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"ac8aad9f-5f25-4148-bf94-b105ca2723e5","deepnote_execution_queue":[]}}
