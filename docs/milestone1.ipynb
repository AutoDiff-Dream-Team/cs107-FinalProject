{"cells":[{"cell_type":"markdown","source":"## Milestone 1 \n\n##### Group Members: Victor Avram, Nishu Lahoti, Yuxin Xu, Diego Zertuche\n\n\n\n### Introduction\n\n---\n\nAutomatic differentiation (AD) encompasses a suite of tools used to compute the derivatives of functions, evaluate the functions at specified values, and evaluate the functions' derivates at specified values. In a situation where analytically deriving the derivative of compicated functions is not feasible (within the user's limitations), AD guarantees to return the exact solution in theory. In practice, rounding errors may compound given that AD performs a series of elementary operations.  \n\n### Background\n\n---\n\n\nThe way automatic differentiation works is by taking a possibly complex function and breaking it down into a sequence of elementary functions \n(i.e. summation, multiplication, cosine, etc.), where the output or outputs of past elementary functions are fed into the input of the next elementary \nfunction. The sequence of the elementary functions starts by first assigning a value or values to the variables in the function, \nthen working its way from the inside out of the function by sequentially performing the elementary functions until you build out the whole function. \nThis sequence can be expressed in a graph structure. ![Graph Image](https://blog.paperspace.com/content/images/2019/03/computation_graph_forward.png) \nOnce you have the sequence of elementary functions, what automatic differentiation does is \"passing down\" the evaluation of the elementary function and the evaluation \nof the derivate through the sequence to get the whole function and its derivative evaluated at certain values. Mathematically, to evaluate a elementary function node, \nyou need to take the function evaluation outputs of the nodes that feed into it and use that to evaluate that node. There is more subtlety for passing along the derivative evaluation. \nIn a way, each elementary function has some variables that it depends on, but those variables from the previous nodes depend on other variables and thus, to pass the derivative along \nthe sequence we need to use the chain rule $$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial x}$$. Now we can see that we need \nto take the derivate of the node we are in and multiply it by the evaluated derivative(s) of the previous node(s)! But we still need a derivate evaluated for the initial mode, \nand this will be assigned with a seed vector of the choosing. Intuivitely, what this seed vector is doing is making the derivative into a directional derivative,\nthe seed vector being the direction where the derivative (or the Jacobian in the case of multiple functions) is being projected in.\n\n\n\n### Software Organization\n\n---\n\n#### Directory Structure\n\n\n    Auto_diff/\n\n        __init__.py  \n        preprocessing/ \n            __init__.py \n            parse_func_inputs.py\n                           ...\n        calculations/\n            __init__.py\n            trace.py  # create evaluation trace and calculate the numerical outputs\n                     ...\n        tests/\n            __init__.py \n            test_parse.py  # test parse_func_inputs module\n            test_trace.py  # test trace module\n                   ...\n                   \n#### Modules\n\nWe will have four modules within our package `Auto_diff`.\n*  `parse_func_inputs`: a module that parse `n` functions into `n` lists of elementary mathematical operations\n*  `trace`: a module that takes in the parsed functions and the seed vectors, creates the evaluation trace and calculates the numerical outputs\n*  `test_parse`: a module that tests parse_func_inputs module \n*  `test_trace`: a module that tests trace module\n\n#### Testing\n\nWe will run a series of tests to ensure the user has input the correct information. The tests will kick back exceptions with printed feedback to guide the user if they do make errors. \n* **Input Variables: ** The user must provide accurate input variables. These are numeric values which are not imaginary.\n* **Input Functions: ** The user must provide functions for which derivatives will be computed. The team is still deciding whether or not we'll allow the user to provide string values which will be parsed or if the user will be required to input numeric, python syntactic functions.\n* **Seed Vectors (optional): ** The user may also input a list of seed vectors to serve as a point which the package will compute the derivative. If the user does not provide a seed vector, we will default to the unit vector.\n\nOur test suite will be included in the subpackage `tests`. We will be using TravisCI and CodeCov.\n\n#### Distribution\nWe are considering two methods for distributing our package.\n\n* **Option 1 - Clone Github Repo: ** Simply allow the user to access our `Auto_diff` package on GitHub and clone into their local repo through a URL.\n* **Option 2 - pip install: ** Create a terminal installer through which the user can use the command line to install our package through pip.\n\nWe will be using `PyPI` for our distribution.\n\n\n#### Packaging and How to use *PackageName*\nOur users will interact with our package รก la SKlearn, where they will load the AD class (from trace module) and instantiate an AD object which they will then fit with a function or a vector of functions.\n\n    myAD = AD()\n    myAD.fit(function)\n\nAfter having the AD object fitted with the function(s), the user will be able to call different methods to get the result they want (derivate and function evaluated at given point, jacobian matrix, etc.).\n\n    f, f_prime = myAD.auto_diff(values)\n\nThe user will have the option of specifying what seed vector to use and which type of differentation to use, as well as refitting the same object with another function or set of functions. \n### Implementation\n---\nClasses: \nThere will be one main class that users will interact with in order to perform AD. The `AD` class is capable of performing the forward mode and \nreverse mode of automatic differentiation. The basic workflow is as follows: The user instantiates an instance of the `AD` class, defines a function,\nand either performs the forward or reverse mode of automatic differentiation. The \n\nAttributes:\n* **function**\n    * Type: Numeric (initially 0)\n    * The function the user wants to analyze.\n* **solution** \n    * Type: Numeric tuple (initially (0,0))\n    * The value of the function and the value of the derivative of the function evaluated using the user input.\n\nMethods:\n* **fit** \n    * INPUT: `function` \\[String\\] The function that the user wants to evaluate. This must be a properly defined function. <br/>\n             `method` \\[String\\] Either \"forward\" or \"reverse\". This parameter defines the mode of AD to perform. Default is \"forward.\n    * OUTPUT: None\n    * This method sets the **function** attribute\n* **auto_diff**\n    * INPUT: `value` \\[Numeric\\] The input value for the function $f$.\n    * OUTPUT: \\[Numeric Tuple\\] The value of $f$ and $f'$ evaluated at the user input.\n    * This method performs automatic differentiation using the value input. Either forward mode or reverse mode is implemented depending on the class.\n\nNecessary Dependencies:\n\n`Numpy` and `SymPy` are the two dependencies that will be required in order to properly use our package. \n`Numpy` will be used to handle the elementary functions such as $sin(x)$ and $exp(x)$.\n`SymPy` will be used to write symbolic functions from the user defined function.\n","metadata":{"tags":[],"cell_id":"00002-bc0135bc-fa06-458e-8746-41e81aa11567"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00001-56d7229e-df99-43b2-a94d-4942ba91cc41"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"8aff4fc1-b0d3-4c41-9b07-f9a1ea53de10","deepnote_execution_queue":[]}}