{"cells":[{"cell_type":"markdown","source":"## Automatic Differentiation Documentation","metadata":{"tags":[],"cell_id":"00000-e180ff43-6804-45d7-9bf0-c9755cd2b7e1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Introduction\n\n---\n\nAutomatic differentiation (AD) encompasses a suite of tools used to compute the derivatives of functions, evaluate the functions at specified values, and evaluate the functions' derivates at specified values. In a situation where analytically deriving the derivative of compicated functions is not feasible (within the user's limitations), AD guarantees to return the exact solution in theory. In practice, rounding errors may compound given that AD performs a series of elementary operations.  \n\n\n","metadata":{"tags":[],"cell_id":"00001-c7f7893c-d0be-4d7e-b0d4-3d7c3059a64f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Background\n\n---\n\n\nThe way automatic differentiation works is by taking a possibly complex function and breaking it down into a sequence of elementary functions \n(i.e. summation, multiplication, cosine, etc.), where the output or outputs of past elementary functions are fed into the input of the next elementary \nfunction. The sequence of the elementary functions starts by first assigning a value or values to the variables in the function, \nthen working its way from the inside out of the function by sequentially performing the elementary functions until you build out the whole function. \nThis sequence can be expressed in a graph structure. ![Graph Image](https://blog.paperspace.com/content/images/2019/03/computation_graph_forward.png) \nOnce you have the sequence of elementary functions, what automatic differentiation does is \"passing down\" the evaluation of the elementary function and the evaluation \nof the derivate through the sequence to get the whole function and its derivative evaluated at certain values. Mathematically, to evaluate an elementary function node, \nyou need to take the function evaluation outputs of the nodes that feed into it and use that to evaluate that node. There is more subtlety for passing along the derivative evaluation. \nIn a way, each elementary function has some variables that it depends on, but those variables from the previous nodes depend on other variables and thus, to pass the derivative along \nthe sequence we need to use the chain rule $$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial x}$$. Now we can see that we need \nto take the derivate of the node we are in and multiply it by the evaluated derivative(s) of the previous node(s)! But we still need a derivate evaluated for the initial mode, \nand this will be assigned with a seed vector of the choosing. Intuivitely, what this seed vector is doing is making the derivative into a directional derivative,\nthe seed vector being the direction where the derivative (or the Jacobian in the case of multiple functions) is being projected in.\n\nFor example, a user may want to evaluate the derivative of a complicated function $f'$ at a given point. Let us define a function $f$:\n$$\nf(x) = sin^3(x^2 + cos(\\sqrt{x}))\n$$\nThe derivative of the function $f'$, given below, is messy and tedious to derive.\n$$\nf'(x) = 3 \\left(2x - \\frac{sin(\\sqrt{x})}{2\\sqrt{x}}\\right) cos(x^2 + cos(\\sqrt{x})) sin^2(x^2 + cos(\\sqrt{x})) \n$$\nHowever, the user does not have to analytically derive the derivative of the given function when using automatic differentiation. \nProvided the user supplies a function of interest and point(s) of interest, the derivative of the given function will be evaluated\nat the given point(s) of interest. ","metadata":{"tags":[],"cell_id":"00002-975f81a1-87cd-40b0-a1ef-534344db58d4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Software Organization\n\n---\n\n#### Directory Structure\n\n\n    Auto_diff/\n\n        __init__.py  \n        fd/\n            __init__.py\n            FD.py  # create FD objects\n                     ...\n        rd/\n            __init__.py\n            RD.py  # create RD objects\n                     ...\n        utils/ \n            __init__.py \n            jacobian.py # helps create jacobian matrix\n                           ...\n        tests/\n            __init__.py \n            test_basic_v2.py  # test basic operations for forward mode \n            test_rd.py  # test basic operations for reverse mode\n            test_jacobian.py  # test jacobian helper \n                   ...\n                   \n#### Modules\n\nWe have three modules within our package `Auto_diff`.\n*  `FD`: a module that contains the following class:\n    * The `FD` class used for instatiating an `FD` object, which is used to perform the forward mode of automatic differentiation and produces the numerical output.\n*  `RD`: a module that contains the following class:\n    * The `RD` class used for instatiating an `RD` object, which is used to perform the reverse mode of automatic differentiation and produces the numerical output.\n*   `jacobian`: a module that contains the following function:\n    * The function`Jacobian` used for handling functions of multiple inputs. This function takes as an argument a list defining the values for each input for the given function and returns a list of `FD` objects.\n*  `test_basic_v2`: a module that tests all the elementary functions (addition, multiplication, power, etc) and functions like `get_value` and `get_derivative` for the forward mode. \n*  `test_rd`: a module that tests all the elementary functions (addition, multiplication, power, etc) and functions like `get_value` and `get_gradient` for the reverse mode. \n*  `test_jacobian`: a module that tests function `Jacobian` on a single function and multiple functions.\n\n\n#### Testing\n\nModule testing can be found in the files `test_basic_v2.py`, `test_rd.py` and `test_jacobian.py`. \n* ** test_basic: ** Each elementary function for the forward mode is tested individually inside `test_basic_v2.py`.\n* ** test_rd: ** Each elementary function for the reverse mode is tested individually inside `test_rd.py`.\n* ** test_jacobian: ** Function `Jacobian` is tested on a single function and multiple functions respectively to see if the function works in both scenarios.\n\nOur test suite is included in the subdirectory `tests` that runs with pytest automatically on TravisCI and CodeCov. ","metadata":{"tags":[],"cell_id":"00003-856770e8-db83-4028-aae0-bc86db87a814","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Installation\n\n##### Installing Python\nYou will need an updated version of python that is compatible with your system. These downloads can be found [here](https://www.python.org/downloads/).\nDownloading a python version $\\geq$ 3.4 will also install pip, the package manager for python.\n\n#### Option 1 - Installing from GitHub\n##### Installing Git\nGit is a version control software that will be used in order to pull all relevant package data from the Github repository. This step is not\nnecessary, but it greatly simplifies the process of downloading all relevant data. The steps used to install git for your given machine\ncan be found [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\nThe automatic differentiation package can be installed by cloning the necessary github repository using the following command.\n\n    git clone https://github.com/AutoDiff-Dream-Team/cs107-FinalProject.git\n\n\nThe dependencies needed to properly use this pacakge can be installed by running the following command. First, you must be located in the directory that contains the `requirements.txt` file.\n\n    pip install -r requirements.txt\n\n\n#### Option 2 - Installing using pip\nThe automatic differentiation package can be installed by using the following command.\n\n    pip install iamautodiff\n\nThe above command will install our package and the necessary dependencies. \n\nThe package `Auto_diff` contains 3 modules `FD`, `RD`, and `jacobian`. These can be imported all at once as follows.\n\n    from Auto_diff import *\n\nThese can also be imported individually as follows.\n\n    from Auto_diff import FD\n    from Auto_diff import RD\n    from Auto_diff import jacobian\n","metadata":{"tags":[],"cell_id":"00004-c38ae4fa-cf96-4380-b604-5a54491958da","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Implementation\n---\nClasses: \nThere will be one main class that users will interact with in order to perform AD. The `FD` class is capable of performing the forward mode of automatic differentiation. The basic workflow is as follows: \nThe user instantiates an instance of the `FD` class by setting the value of the given variable and uses the newly created `FD` object as the input to a user-defined function. \nThe `FD` object stores the value of the function and the value of the derivative in the attributes `val` and `der`. The `RD` class performs\nthe reverse mode of automatic differentiation. The user instantiates an instance of the `RD` class, similar to instantiation of an `FD` instance.\n\n<u>Inputs, attributes, and methods for an `FD` object:</u>\n\nInputs:\n* **val**\n    * Type: Numeric (default is 1)\n    * The value of the function evaluated at the specified user input for the given variable\n* **der** \n    * Type: Numeric (default is 1)\n    * The value of the derivative of the function evaluated at the specified user input\n    * This input is likely to be changed from 1 when computing partial derivatives for functions of multiple variables\n\nAttributes:\n* **val**\n    * Type: Numeric (default is initially 1)\n    * The value of the function evaluated at the specified user input\n* **der** \n    * Type: Numeric (default is initially 1)\n    * The value of the derivative of the function evaluated at the specified user input\n\nMethods:\n* The `FD` class does not contain methods that will be commonly accessed by the user.\n* Basic operations including addition, subtraction, multiplication, division, power, exponential, negation, and the trigonometric functions sine, cosine, and tangent\n  are overloaded in the definition of the class.\n* Methods that will be explicitly called from the class include:\n    * `logarithm`: The first argument is a `FD` object and the second argument is the base\n    * `logistic`: The only argument is a `FD` object.\n    * `get_derivatives`: Returns the derivative with respect to the given variable for a list of `FD` objects, evaluated at the value of the given variable.\n    * `get_values`: Returns the value of the given `FD` object for a list of `FD` objects.\n\n<u>Inputs, attributes, and methods for an `RD` object:</u>\n\nInputs:\n* **val**\n    * Type: Numeric (default is 1)\n    * The value of the function evaluated at the specified user input for the given variable\n\nAttributes:\n* **val**\n    * Type: Numeric (default is initially 1)\n    * The value of the function evaluated at the specified user input\n* **grad**\n    * Type: Numeric (default is 1)\n    * The partial derivative of the given function with respect to the given variable\n* **children**\n    * Type: List of tuples\n    * The first element of each tuple is the derivative of the elementary function. The second element of each tuple is a `RD` object which is a child of the given object. Traversal over these children is used for the reverse mode.\n\nMethods:\n* Most methods from the `RD` class will not be commonly accessed by the user.\n* Basic operations including addition, subtraction, multiplication, division, power, exponential, negation, and the trigonometric functions sine, cosine, and tangent\n  are overloaded in the definition of the class.\n* Methods that will be explicitly called from the class include:\n    * `logarithm`: The first argument is a `RD` object and the second argument is the base\n    * `logistic`: The only argument is a `RD` object.\n    * `get_gradient`: Returns the derivative with respect to the given variable evaluated at the value of the given variable.\n    * `get_value`: Returns the value of the given variable.\n\nAdditionally, the `Jacobian` function can be used in order to streamline the process of computing the Jacobian matrix. Given the input \nvalues for the variables of a given function passed as a list when using the `Jacobian` function, a 2D array of `FD` objects is returned.\nThis allows users to compute the Jacobian for a given function without needing to manually compute each element of the Jacobian. \n\n<u>Inputs, attributes, and methods for a `Jacobian` object:</u>\n\nInputs:\n* **arr**\n    * Type: Numeric List\n    * The value of each input variable for the given function or functions\n\nOutput:\n* Type: 2D numpy array\n* Each element in the 2D numpy array is an `FD` object\n\nNecessary Dependencies:\n\n`Numpy` is the only dependency that will be required in order to properly use our package. \n`Numpy` will be used to handle the elementary functions such as $sin(x)$ and $exp(x)$.\n","metadata":{"tags":[],"cell_id":"00004-3bd5f617-f224-4423-8452-dd6fa3c36b71","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### How to Use\n---","metadata":{"tags":[],"cell_id":"00006-83187a7e-3aa8-47c7-beb9-b9a29c9e6c78","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Instantiating and using an FD object (Forward Mode of Automatic Differentiation)","metadata":{"tags":[],"cell_id":"00006-08f4e063-b84f-465f-9725-6f42347030fc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Import the FD module and numpy using the following commands. \n\n    \n    from Auto_diff import FD\n    import numpy as np","metadata":{"tags":[],"cell_id":"00007-c3dae577-c5c0-4492-8346-c09982ea07d4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Instantiate an FD object. You can change the first argument, but leave the second argument as 1 for this example.\n    \n    # Instantiates the FD object\n    val = 3\n    x = FD(val,1)","metadata":{"tags":[],"cell_id":"00009-95f22758-4f2f-41cd-a90e-a1e7d18066e3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Define a function that takes one argument as input. This will be your FD object.\n\n    \n    # User defined function (you can change the body of this function)\n    def f(x):\n        output = np.sin(x**2 + 3)\n        return output","metadata":{"tags":[],"cell_id":"00011-9318cef3-44a3-4ebf-af09-9f9de61b19c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Once the function is defined, you can call the function by passing the FD object as the argument.\n\n    \n    # Running this will change the values of the val and der attributes (these are shown in output which is an FD object)\n    output = f(x)\n\n    # If you dont wan't to define a function you can compute them on the fly\n    output = np.sin(x**2 + 3)\n\n    print(f\"The value of the function evaluated at {val} is {output.val}.\")\n    print(f\"The value of the derivative of the function evaluated at {val} is {output.der}.\")","metadata":{"tags":[],"cell_id":"00013-2f3d36c8-6b6f-412d-8ffb-a2be12a78b30","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Using multiple FD objects (functions of multiple inputs)","metadata":{"tags":[],"cell_id":"00015-f8ac556d-1538-46d4-898b-31b2accfda1e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"You can define functions of multiple variables where each variable is an FD object. An example is given below. When using multiple variables\nthe second arguments in the instantiations of the AD objects will define the seed vector and therefore will dictate the value of the computed \nderivative. For example, let us define a function $f=x^2+y$. Let us set the values for $x$ and $y$ as 2 and 3 respectively.\nWe can instantiate the FD objects as follows and define our function as follows.\n\n    # Instantiates the FD objects\n    x = FD(2,1) \n    y = FD(3,1) \n\n    # Define the function\n    def f(x,y):\n        return x**2 + y\n\n","metadata":{"tags":[],"cell_id":"00016-38a910c5-b89c-4bdb-b47b-7d18da97a066","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We will first manually derive the value of the function and the values of the partial derivatives of the function.\n\nWe can evaluate the function at $x=2$ and $y=3$. \n$$\nf(2,3) = 2^2 + 3 = 7\n$$\nWe can also determine $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$.\n$$ \n\\frac{\\delta f}{\\delta x} = 2x\n$$\n$$ \n\\frac{\\delta f}{\\delta y} = 1  \n$$\nUsing our values for $x$ and $y$, $\\frac{\\delta f}{\\delta x}=4$ and $\\frac{\\delta f}{\\delta y}=1$.\nWe can compute $\\frac{\\delta f}{\\delta x}$ by setting the second argument for `x` to 1 and the second\nargument for `y` to 0. Similarly, we can compute $\\frac{\\delta f}{\\delta y}$ by setting the second argument \nfor `x` to 0 and the second argument for `y` to 1. This is done as follows.\n\n    # Evaluate the function and the partial derivative with respect to x for x=2 and y=3\n    x = FD(2,1) \n    y = FD(3,0) \n    output1 = f(x,y)\n\n    # Evaluate the function and the partial derivative with respect to y for x=2 and y=3\n    x = FD(2,0) \n    y = FD(3,1) \n    output2 = f(x,y)\n\n    # The value of the function should be the same for both evaluations\n    function_value1 = output1.val\n    function_value2 = output2.val\n    assert function_value1 == function_value2\n\n    der_x = output1.der\n    der_y = output2.der\n\n    print(f\"The value of the function f at x=2 and y=3 is {function_value1}\".)\n    print(f\"The partial derivative with respect to x of f at x=2 and y=3 is {der_x}\".)\n    print(f\"The partial derivative with respect to y of f at x=2 and y=3 is {der_y}\".)","metadata":{"tags":[],"cell_id":"00018-ffab7377-600b-4112-aeff-3155f1ee583e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Computing the Jacobian","metadata":{"tags":[],"cell_id":"00014-1c5c6856-8007-4432-8012-66db6085c603","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We also give the user the option of directly taking the Jacobian Matrix of a function or set of functions with one or multiple variables. The user will\nimport the Jacobian module and call the Jacobian method by passing just the values of the variables they want and the method will return a numpy ndarray \nof FD objects ready to be used in a single function or in a list of functions. This will return a matrix of n x n of FD objects evaluated at the \ncorrect seed vectors for the Jacobian. You can get only the derivatives by using the get_derivative method.\n    \n    from Auto_diff import Jacobian, FD\n\n    # Passing in list of values for x amount of variables\n    x = Jacobian([2, 7, 10])\n\n    # This assignment will return a 3x3 matrix of AD objects\n    jacobian_results = [np.exp(x[0]/x[1] + x[2]),tan(sin(3^x[0])*x[2]), x[0]+x[1]+x[2]]\n\n    # Getting only the derivatives of the 3x3 matrix, i.e., the Jacobian matrix\n    jacobian_matrix = FD.get_derivatives(jacobian_results) \n\n    print(jacobian_matrix)","metadata":{"tags":[],"cell_id":"00014-a2599165-4a59-4de2-b0a3-203a091ed219","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Reverse Mode\n\n","metadata":{"tags":[],"cell_id":"00020-28df31e3-80fa-4f3b-a20e-41ae22bc75d7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We implemented the reverse mode of automatic differentiation. This implementation is meant to overcome one of the main pitfalls of \nthe forward mode, namely computing multiple partial derivatives such as has to be done when computing the Jacobian. For example, given \nwe want to compute $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ for some function $f$, we would need to perform the forward\nmode twice. Once when setting the seed for the `FD` object associated with $x$ to 1 and the `FD` object associated with $y$ to 0 and another\ntime setting the seed for the `FD` object associated with $x$ to 0 and the `FD` object associated with $y$ to 1. \n\nWe can think of performing the reverse mode as inverting the expressions for the derivatives when peforming the chain rule. Our implementation\nperforms a forward pass through the mathematical expression, where each elemental operation is represented as a node, storing the 'children'\nof each node. After carrying out the forward pass, we do a backwards pass throught all the elemental operations carrying the gradient through\nthem until reaching the input variable we want to take the gradient of. Once the forward pass is done, we can take the gradient of any number of \ninput variables easily without having to instatiate new objects. It has to be noted that, different from forward differentation, once a Reverse\nDifferentation object is used for particular function, this object can't be reused for another function. More precisely, if `n` is the number of input\nvariables and `m` is the number of functions, if we are using Forward Differentation we have to create and carry out the forward pass `n` number of times, \nwhile if we perform Reverse Differentation, we carry out the process m number of times.\n\n","metadata":{"tags":[],"cell_id":"00018-92f7a0ec-df71-4e6c-8b20-3306c8c9284e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Instantiating and using an RD object (Reverse Mode of Automatic Differentiation)","metadata":{"tags":[],"cell_id":"00019-070e1bbb-3282-42af-abdd-add76027764a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Peforming the reverse mode of automatic differentiation is similar to performing the forward mode previously outlined. Below is a quick\ntutorial on how to perform the reverse mode.","metadata":{"tags":[],"cell_id":"00021-aef017df-30db-4531-9734-ec8000204ed9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"    from Auto_diff import FD\n    import numpy as np","metadata":{"tags":[],"cell_id":"00022-814029ba-38cc-4d9a-82d8-d7d5823d407e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"    # Instantiates the FD object\n    val_x = 3\n    val_y = 5\n    val_y = 10\n    x = RD(val_x)\n    y = RD(val_y)\n    z = RD(val_z)\n\n    # User defined function (you can change the body of this function)\n    def f(x,y,z):\n        output = np.sin(x**2 + 3) + np.tan(y/z)\n        return output\n\n    # Running this will change the value of the grad attrubute of x, y, and z\n    output = f(x,y,z)\n\n    # If you dont want to define a function you can compute them on the fly\n    output = np.sin(x**2 + 3) + np.tan(y/z)\n\n    # You can pull the values of x, y, and z the value of the function, and the derivative of the function with respect to x, y, and z\n    val_x = x.get_value()\n    val_y = y.get_value()\n    val_z = z.get_value()\n    val_f = output.val\n    grad_x = x.get_gradient()\n    grad_y = y.get_gradient()\n    grad_z = z.get_gradient()\n\n    print(f\"The values of x, y, and z are {val_x}, {val_y}, and {val_z} respectively.\")\n    print(f\"The value of the function evaluated at {val_x}, {val_y}, and {val_z} is {val_f}.\")\n    print(f\"The value of the derivative of the function with respect to x is {grad_x}.\")\n    print(f\"The value of the derivative of the function with respect to y is {grad_y}.\")\n    print(f\"The value of the derivative of the function with respect to z is {grad_z}.\")","metadata":{"tags":[],"cell_id":"00023-463e1ff6-0c16-4af0-bbd4-e64844d63fc3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"As can be seen, the `get_gradient()` method can be called on each `RD` object in order to get the derivative with respect to the given \nvariable. Only one forward pass per function is performed. One reverse pass is performed for each variable (this is done when using the\n`get_gradient()` method).","metadata":{"tags":[],"cell_id":"00023-0b9003b2-6fb8-4b04-8ae8-6f2f8af52608","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Compatible Functions\n---","metadata":{"tags":[],"cell_id":"00023-dd01f80f-4d9d-4d71-ab50-6d9f00aa5498","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Both of our FD and RD classes are compatible with the following functions:\n- np.sin, np.arcsin, np.sinh\n- np.cos, np.arccos, np.cosh\n- np.tan, np.arctan, np.tanh\n- np.exp\n- np.sqrt\n- FD.logarithm, RD.logarithm (both function take an input `base`, ex. FD.logarithm(my_FD, np.e), this will give a logarithm with base $e$)\n- FD.logistic, RD.logistic (logistic function)","metadata":{"tags":[],"cell_id":"00024-6b2017d8-8149-4b3f-b631-9c79c12c7ef6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Broader Impact and Inclusivity Statement\n---","metadata":{"tags":[],"cell_id":"00019-3e5d9a16-3d22-44b8-8777-57237699e9fb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"In the closing years of the 2010s, the technology industry came under fire for turning a blind eye to the unintended consequences created from new-age technology. The classic example is that of Facebook and the divisiveness the United States experienced in the 2016 and 2020 elections. This statement aims to reflect upon the issue how our automatic differentiation package could have unintended, exclusionary consequences.\n\nOur development team operated under the assumption that users of our package have a basic familiarity with object oriented programming, calculus and mathematical terminologies in English. While we built the package to have a smooth user experience, the software functions inside of a python environment, and rides upon the basic assumption that users have fundamental programming, mathematical capabilities and some basic english. While this was by design, the package does, as a result, exclude anyone without these fundamental abilities, of which there is a large portion of the United States / Cambridge / Harvard community. In the case that a student or professional aimed to solve a complex automatic differentiation problem, and did not have basic capabilities in python, they would be excluded from our package.\n\nA potential solution that would make our package more inclusive is the development of a web interface , similar to the one demo'd in class, through which any user hoping to compute a complex derivative to machine precision could simply enter their input functions and values and get a quick and easy result. This could be an extension for the future of our package.\n\nWe had some additional considerations on how this package could be more inclusive. To start, we could expand it beyond python and make it accessible in different languages. This package is meant for python developers, a subset of the coding population which may be relatively young and early-career. For developers later in their career that have not necessarily learned python (or earlier, for that matter) we would need a package that speaks their native coding language. In this way, we could expand our software. ","metadata":{"tags":[],"cell_id":"00024-70fcf9a8-8626-4a36-b620-fdc56ade1f1e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Future\n---\n\nThere are many more extensions that will be a great add-on to our package in the future: \n\n1. adding functions such as factorial, cube root, gamma function, permutation, combination, etcetera.\n\n2. implementation for complex numbers\n\n3. implementation for higher order derivatives or mixed derivatives\n* This is useful for modeling many physical phenomena\n\n4. additional features for root finding\n* This is useful for optimization problems that arise in machine learning, biomedical research, finance, etc. Optimization problems\nare found in almost every field. Root finding is an obvious complement to automatic differentiation.\n\n5. implementation for non-differentiable functions\n\n\n\n\n\n\n","metadata":{"tags":[],"cell_id":"00025-7063d3df-0be9-46dd-9933-20243df39c02","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00029-38a7ab5f-e2f9-4530-9aac-248b2b6a729c","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"ac8aad9f-5f25-4148-bf94-b105ca2723e5","deepnote_execution_queue":[]}}